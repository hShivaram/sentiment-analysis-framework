name: CI

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run integration tests daily at midnight UTC
    - cron: '0 0 * * *'
  workflow_dispatch:  # Allow manual triggering

# Cache for model weights to speed up integration tests
env:
  TRANSFORMERS_CACHE: ${{ github.workspace }}/.cache/huggingface/hub
  HF_HOME: ${{ github.workspace }}/.cache/huggingface
  PYTHON_VERSION: '3.12'

jobs:
  unit-tests:
    name: Run Unit Tests
    runs-on: ubuntu-latest
    outputs:
      coverage-file: ${{ steps.coverage.outputs.file }}

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e '.[test]'  # Install package with test extras
    
    - name: Run unit tests
      id: coverage
      run: |
        pytest tests/test_sentiment_model.py tests/test_sentiment_edge_cases.py -v --cov=src --cov-report=xml
        echo "file=coverage.xml" >> $GITHUB_OUTPUT
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

  integration-tests:
    name: Run Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    # Run integration tests on schedule, release, workflow_dispatch, or when labeled
    if: >
      (github.event_name == 'schedule' ||
      github.event_name == 'release' ||
      github.event_name == 'workflow_dispatch' ||
      contains(github.event.pull_request.labels.*.name, 'run-integration-tests')) &&
      needs.unit-tests.result == 'success'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      id: setup-python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'  # Using Python 3.11 for compatibility with deep-eval
    
    - name: Cache model weights
      uses: actions/cache@v3
      id: cache-hf
      with:
        path: ${{ env.HF_HOME }}
        key: ${{ runner.os }}-huggingface-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-huggingface-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # Install deep-eval directly from GitHub
        pip install git+https://github.com/confident-ai/deepeval.git
        # Then install the rest of the package with test extras
        pip install -e '.[test]'  # Install package with test extras
    
    - name: Run integration tests
      run: |
        pytest tests/test_sentiment_model_deepeval.py -v --cov=src --cov-append --cov-report=xml
      env:
        # Increase timeout for model downloads
        HUGGINGFACE_HUB_CACHE: ${{ env.TRANSFORMERS_CACHE }}
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

  model-validation-tests:
    name: Run Model Validation Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    
    # Run on schedule, release, workflow_dispatch, or when labeled
    if: >
      (github.event_name == 'schedule' ||
      github.event_name == 'release' ||
      github.event_name == 'workflow_dispatch' ||
      contains(github.event.pull_request.labels.*.name, 'run-model-validation')) &&
      needs.integration-tests.result == 'success'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
    
    - name: Cache model weights
      uses: actions/cache@v3
      id: cache-hf
      with:
        path: ${{ env.HF_HOME }}
        key: ${{ runner.os }}-huggingface-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-huggingface-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e '.[test]'  # Install package with test extras (includes deepchecks)
    
    - name: Run model validation tests
      run: |
        pytest tests/test_model_with_deepchecks.py -v
      env:
        HUGGINGFACE_HUB_CACHE: ${{ env.TRANSFORMERS_CACHE }}

  # This job runs after any test job completes
  report-coverage:
    name: Report Coverage
    needs: 
      - unit-tests
      - integration-tests
      - model-validation-tests
    runs-on: ubuntu-latest
    # Run if any test job was attempted (success or failure)
    if: >
      always() && 
      (
        (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'failure') ||
        (needs.integration-tests.result == 'success' || needs.integration-tests.result == 'failure') ||
        (needs.model-validation-tests.result == 'success' || needs.model-validation-tests.result == 'failure')
      )
    
    steps:
    - name: Upload coverage report
      uses: codecov/codecov-action@v3
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        file: ./coverage.xml
        fail_ci_if_error: false
        verbose: true
